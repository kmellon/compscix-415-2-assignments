---
title: "COMPSCIX 415.2 Homework 9/Final"
author: "Kate Mellon"
date: "3/27/2018"
output:
  html_document:
    toc: true
    toc_depth: 5
---

#### Bootstrapping

1.)

```{r load_packages, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(modelr)
library(randomForest)
library(tidyverse)
train <- read.csv("/users/katie/downloads/trainhw8.csv")

train <- train %>% mutate(Survived = as.factor(Survived))

glimpse(train)

```

2.)

```{r}
library(tidyverse)
library(modelr)
titanic_boot <- bootstrap(data = train, n = 100)

```


3.)

```{r}
# since the strap column of titanic_boot is a list, we can
# extract the resampled data using the double brackets [[]],
# and just pick out a few of them to compare the number of
# distinct rows

as.tibble(titanic_boot$strap[[1]]) %>% n_distinct()
as.tibble(titanic_boot$strap[[2]]) %>% n_distinct()
as.tibble(titanic_boot$strap[[3]]) %>% n_distinct()

```


4.)

```{r}

library(tidyverse)

age_mean <- function(titanic_boot) {
data <- as.tibble(titanic_boot) # convert input data set to a tibble
mean_age <- mean(data$Age, na.rm = TRUE) # take the mean of Age, remove NAs
return(mean_age) # return the mean value of Age from data
}
# loop through the 100 bootstrap samples and use the age_mean()
# function
all_means <- rep(NA, 100)

# start the loop
for(i in 1:100) {
all_means[i] <- age_mean(titanic_boot$strap[[i]])
}
# take a look at some of the means you calculated from your samples
head(all_means)
# convert to a tibble so we can use if for plotting
all_means <- tibble(all_means = all_means)

```


5.)

```{r}
ggplot(all_means, mapping=aes(all_means)) +
  geom_histogram(binwidth = .5) +
  xlab("Mean Age") +
  ylab("Count of Mean Age")
```


6.)

```{r}
(empirical_se <- all_means %>% summarize(se_age = sd(all_means)))

(theoretical_se <- sd(train$Age, na.rm = TRUE)/sqrt(100))
```

The empirical standard error of 0.54 using the bootstrapping method is lower than the theoretical standard error of 1.45.


#### Random Forests


1.)

```{r}
set.seed(987)
model_data <- resample_partition(train, c(test = 0.3, train = 0.7))

train_set <- as.tibble(model_data$train)
test_set <- as.tibble(model_data$test)

```

2.)

```{r}
library(rpart)
library(partykit)

tree_mod <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = train_set)
plot(as.party(tree_mod))

```

Compared to the tree last week, this tree has more nodes and reveals more detail on the factors which determined survival. For instance, last weeks chart showed that being male had a low probability of survival, while this chart reveals that male children had a much higher likelihood of survival. Of the new factors added, Age is the only which appears in the tree.

3.)

```{r}
library(randomForest)
rf_mod <- randomForest(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
data = train_set,
ntrees = 500,
mtry = 4,
na.action = na.roughfix)
```


4.) 

```{r}
library(ROCR)
rf_preds <- predict(rf_mod, newdata = test_set, type = 'prob')[,2]
tree_preds <- predict(tree_mod, newdata = test_set)[,2]
pred_rf <- prediction(predictions = rf_preds, labels = test_set$Survived)
pred_tree <- prediction(predictions = tree_preds, labels = test_set$Survived)

```


5.)

```{r}

# get the FPR and TPR for the logistic model
# recall that the ROC curve plots the FPR on the x-axis
perf_rf <- performance(pred_rf, measure = 'tpr', x.measure = 'fpr')
perf_rf_tbl <- tibble(perf_rf@x.values[[1]], perf_rf@y.values[[1]])
# Change the names of the columns of the tibble
names(perf_rf_tbl) <- c('fpr', 'tpr')
# get the FPR and TPR for the tree model
perf_tree <- performance(pred_tree, measure = 'tpr', x.measure = 'fpr')
perf_tree_tbl <- tibble(perf_tree@x.values[[1]], perf_tree@y.values[[1]])
# Change the names of the columns of the tibble
names(perf_tree_tbl) <- c('fpr', 'tpr')

# Plotting function for plotting a nice ROC curve using ggplot

plot_roc <- function(tbl1,tbl2) {
rf_tree <- bind_rows(tbl1, tbl2)
p <- ggplot(data = rf_tree, aes(x = fpr, y = tpr)) +
geom_line(color = 'blue') +
geom_abline(intercept = 0, slope = 1, lty = 3) +
labs(x = 'False positive rate', y = 'True positive rate') +
theme_bw()
return(p)
legend("right", legend = c("Decision_Tree", "Random_Forest"), col = c('red','blue')) 
}

plot_roc(perf_tree_tbl, perf_rf_tbl)

# calculate the AUC
auc_rf <- performance(prediction.obj = pred_rf, measure = 'auc')
auc_tree <- performance(prediction.obj = pred_tree, measure = 'auc')

# extract the AUC value
auc_rf@y.values[[1]]
auc_tree@y.values[[1]]

```

6. Answer these questions about the ROC curves:

Which model performs better: decision tree or random forest?

The random forest model performs better as it has a higher auc than the decision tree model, and the ROC curves illustrate that a rising true positive rate has a higher false positive rate on the decision tree compared to the random forest. 

What is the approximate false positive rate, for both the decision tree and the random forest, if we attain a true positive rate of approximately 0.75? Answers do not need to be exact - just ballpark it by looking at the plots.

The approximate false positive rate for the decision tree is 0.34 and for the random forest is 0.15.



